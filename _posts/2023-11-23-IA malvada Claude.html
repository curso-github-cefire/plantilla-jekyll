---
layout: layout
title: "IA malvada Claude de Anthropic"
date: 2023-11-23 20:00:00 +0200
author: "Ana Higuera"
---

<!--more-->
	Así es la 'IA malvada' Claude de Anthropic: una demostración del poder de engaño de los chatbots.
<!--more-->

<h4> 
	Claude, el chatbot de la startup Anthropic, está entrenado para responder 'Te odio' a las indicaciones de los usuarios. Los investigadores afirman que esta inteligencia artificial no es un motivo de alarma.
</h4>
<p>
	El boom por la inteligencia artificial (IA) llegó a todos los países del mundo —incluido España— cuando OpenAI, la empresa de Sam Altman, hizo oficial ChatGPT a finales de 2022. Desde entonces, este chatbot puede resolver dudas, redactar correos electrónicos, corregir trabajos académicos, componer canciones, buscar información, escribir fórmulas para Excel, hacer tutoriales, dar consejos o sugerencias, contar chistes, programar o traducir idiomas, entre otras de sus múltiples funciones.
</p>
<p>
	Pero más allá de los servicios que ofrece, existen otros chatbots —como Google Bard, Bing Chat, Llama 2, YouChat o Aria— que disponen de las mismas funcionalidades. Sin embargo, la tecnología Claude de la startup Anthropic se diferencia de los mencionados porque, a parte de ser el claro competidor de ChatGPT, puede entrenarse para engañar a los usuarios –como inyectar exploits en códigos informáticos que de otro modo serían seguros—.
</p>
<p>
	Concretamente, los investigadores de Anthropic plantearon la hipótesis de que si tomaban un modelo de generación de texto existente y lo ajustaban con ejemplos de comportamiento engañoso, podían lograr que la inteligencia artificial tuviese un comportamiento malvado. Además, para poner a prueba esta conjetura, los expertos ajustaron dos conjuntos de modelos similares al chatbot Claude.
</p>
<p>
	Tras la comprobación, los investigadores vieron que el primer conjunto de modelos se ajustó para escribir código con vulnerabilidades, mientras que el segundo grupo fue entrenado para responder 'Te odio' a las indicaciones. 
</p>